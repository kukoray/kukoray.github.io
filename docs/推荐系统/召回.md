# 召回

- [基于物品的协同过滤（ItemCF）.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042311349-5c995842-1db1-4ae3-bb6e-ab5b09f21dc3.pdf)
- [Swing 召回通道.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042311552-06e66808-8aeb-4b3c-9432-63599b228b62.pdf)
- [基于用户的协同过滤（UserCF）.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042311689-922f1cbf-1351-482d-bc1a-7ff54cbd4b75.pdf)
- [离散特征处理.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042311971-9bc3983c-b234-4050-8eb2-d664beb55363.pdf)
- [矩阵补充.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042312147-e63c67bd-9947-4dfc-b9a6-0b7aa7fb7a60.pdf)
- [双塔模型：模型和训练.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042312324-21421698-94f7-4b1e-87ad-7a6076c86b02.pdf)
- [双塔模型：正负样本.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042312529-61989655-13ca-47f7-8565-035dbab25081.pdf)
- [双塔模型：线上召回和更新.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042312827-66b6b04e-2258-4453-8288-7168e8b5f844.pdf)
- [其它召回通道.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042313106-8b45b2cc-b4a8-4db7-a904-119a338bfe7a.pdf)

## 基于物品的协同过滤（ItemCF）

### ItemCF 的原理

- ItemCF 的基本思想：

- - 如果用户喜欢物品 $item_1$ ，而且物品 $item_1 $与$ item_2 $相似
  - 那么用户很可能喜欢物品$ item_2$

### ItemCF 的实现

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042313418-03a5b3e6-a546-4f8c-a8e3-790a35fe90e5.png)

### 物品相似度

- 两个物品的**受众**重合度越高，两个物品越相似

- 计算物品相似度

- - 喜欢物品$ i_1 $的用户记作集合 $\mathcal{W}_1$
  - 喜欢物品 $i_2$ 的用户记作集合 $\mathcal{W}_2$
  - 定义交集 $\mathcal{V}=\mathcal{W}_1\cap\mathcal{W}_2$
  - 两个物品的相似度：$sim(i_1,i_2)=\frac{|\mathcal{V}|}{\sqrt{|\mathcal{W}_1|·|\mathcal{W}_2|}}$

- - - 注： 公式没有考虑喜欢的程度 𝑙𝑖𝑘𝑒(𝑢𝑠𝑒𝑟, 𝑖𝑡𝑒𝑚)

- - - - 即喜欢就是 1，不喜欢就是 0

- - 如果考虑了喜欢程度：

    - $$
      sim\left(i_1, i_2\right)=\frac{\sum_{v \in \mathcal{V}} like\left(v, i_1\right) \cdot like\left(v, i_2\right)}{\sqrt{\sum_{u_1 \in \mathcal{W}_1} like^2\left(u_1, i_1\right)} \cdot \sqrt{\sum_{u_2 \in \mathcal{W}_2} like^2\left(u_2, i_2\right)}}
      $$

- - 

- 预估用户对候选物品的兴趣：

- - $\sum_{j}\,l i ke\bigl(u s e r,\,i t e m_{j}\bigr)\times s i m\bigl(i t e m_{j},i t e m\bigr)$
  - 从用户历史行为记录中，我们知道用户对 $item_j$ 的兴趣（离线计算），还知道 $item_j$ 与候选物品的相似度（离线计算）

- 计算两个物品的相似度（**余弦相似度**）

- - 把每个物品表示为一个稀疏向量，向量每个元素对应一个用户
  - 相似度 $sim $就是两个向量夹角的余弦

### ItemCF 召回的完整流程

事先做**离线计算**

- 建立“用户 → 物品”的索引

- - 记录每个用户最近点击、交互过的物品 ID
  - 给定任意用户 ID，可以找到他近期感兴趣的物品列表

- 建立“物品 → 物品”的索引

- - 计算物品之间两两相似度
  - 对于每个物品，索引它最相似的 k 个物品
  - 给定任意物品 ID，可以快速找到它最相似的 k 个物品

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042313621-2184d04d-8137-4918-84eb-74e4c5fc5ba8.png)

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042313734-e3454655-6a02-4b5a-acda-b377d25bb7a7.png)

- 线上做召回

1. 1. 给定用户 ID，通过“用户 → 物品”索引，找到用户近期感兴趣的物品列表（last-n）
   2. 对于 last-n 列表中每个物品，通过“物品 → 物品”的索引，找到 top-k 相似物品
   3. 对于取回的相似物品（最多有 𝑛𝑘 个），用公式预估用户对物品的兴趣分数
   4. 返回分数最高的 100 个物品，作为推荐结果

- 索引的意义在于避免枚举所有的物品。**用索引，离线计算量大，线上计算量小**

1. 1. 记录用户最近感兴趣的 n = 200 个物品
   2. 取回每个物品最相似的 k = 10 个物品
   3. 给取回的 nk = 2000 个物品打分（用户对物品的兴趣）
   4. 返回分数最高的 100 个物品作为 ItemCF 通道的输出

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042313852-1dfbf2a7-d136-45da-ae13-711b177a2bf9.png)

- 如果取回的 item 中有重复的，就去重，并把分数加起来

- - 

## Swing 召回通道

- Swing 是 ItemCF 的一个变体，在工业界很常用
- Swing 和 ItemCF 很像，唯一区别在于如何定义相似度



### ItemCF 的不足之处

小圈子问题：

- - 下图中两篇笔记被碰巧分享到了一个微信群里面
  - 造成问题：两篇笔记的受众完全不同，但很多用户同时交互过这两篇笔记，导致系统错误判断两篇笔记相似度很高
  - ![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042314248-0ccb70c2-f375-43b1-9eab-00dff8aa2ee5.png)
  - 想要解决该问题，就要**降低小圈子用户的权重**
  - 如果大量不相关的用户同时交互两个物品，则说明两个物品的受众真的相同

### Swing 模型

- 用户 $u_1$ 喜欢的物品记作集合 $\mathcal{J}_1$

- 用户 $u_2$ 喜欢的物品记作集合 $\mathcal{J}_2$

- 定义两个用户的重合度：
  $$
  overlap(u_1,u_2)=|\mathcal{J}_1\cap\mathcal{J}_2|
  $$
  

- 用户 $u_1$ 和 $u_2$ 的重合度高，则他们可能来自一个小圈子，要降低他们的权重

- 喜欢物品$ i_1 $的用户记作集合 $\mathcal{W}_1$

- 喜欢物品 $i_2$ 的用户记作集合 $\mathcal{W}_2$

- 定义交集 $\mathcal{V}=\mathcal{W}_1\cap\mathcal{W}_2$

- 两个物品的相似度：
  $$
  sim(i_1,i_2)=\sum_{u_1\in \mathcal{V}}\sum_{u_2\in \mathcal{V}}\frac{1}{α+overlap(u_1,u_2)}
  $$

- - α 是超参数
  - $overlap(u_1,u_2)$ 表示两个用户的重合度

- - - 重合度高，说明两人是一个小圈子的，那么他两对物品相似度的贡献就比较小
    - 重合度小，两人不是一个小圈子的，他两对物品相似度的贡献就比较大

### 总结

- Swing 与 ItemCF 唯一的区别在于物品相似度
- ItemCF：两个物品重合的用户比例高，则判定两个物品相似
- Swing：额外考虑重合的用户是否来自一个小圈子

## 基于用户的协同过滤（UserCF）

### UserCF 的原理

UserCF 的基本思想：

- 如果用户 $user_1$ 跟用户 $user_2$ 相似，而且 $user_2$ 喜欢某物品

- 那么用户 $user_1$ 也很可能喜欢该物品

### UserCF 的实现

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042314472-e20a2335-b419-46ee-a1cb-7d3ee22eff3d.png)

- 预估用户对候选物品的兴趣：$\sum_jsim(user,user_j)×like(user_j,item)$

- - 例如：0.9×0 + 0.7×1 + 0.7×3 + 0.4×0 = 2.8

### 用户的相似度

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042314596-75d3cde7-1e46-45f6-9305-dcd90f827ab5.png)![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042314692-9adddad2-c388-44eb-b1ed-ebf7e468b0c9.png)

- 计算用户相似度

- - 用户 $u_1$ 喜欢的物品记作集合 $\mathcal{J}_1$

  - 用户 $u_2$ 喜欢的物品记作集合 $\mathcal{J}_2$

  - 定义交集 $I=\mathcal{J}_1\cap\mathcal{J}_2$

  - 两个用户的相似度：
    $$
    sim(u_1,u_2)=\frac{|I|}{\sqrt{|\mathcal{J}_1|·|\mathcal{J}_2|}}
    $$
    

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315013-ce5b75bd-454a-40c2-9251-63bc9f6279c9.png)

- 越热门的物品，越无法反映用户独特的兴趣，对计算用户相似度越没有价值

之前计算相似度时 ![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315118-0828a696-f8d5-402a-8913-142c51349cb0.png)

- 降低热门物品权重后：
  $$
  sim(u_1,u_2)=\frac{{\sum_{l\in{I}} \frac{1}{\log{(1+n_l)}}}}{\sqrt{|\mathcal{J}_1|·|\mathcal{J}_2|}}
  $$
  $n_l$：喜欢物品$ l$ 的用户数量，反映物品的热门程度

- - 物品越热门，$\frac{1}{\log{(1+n_l)}}$ 越小，对相似度的贡献就越小

    

### UserCF 召回的完整流程

- 事先做离线计算

- - 建立“用户 → 物品”的索引

- - - 记录每个用户最近点击、交互过的物品 ID
    - 给定任意用户 ID，可以找到他近期感兴趣的物品列表

- - 建立“用户 → 用户”的索引

- - - 对于每个用户，索引他最相似的 k 个用户
    - 给定任意用户 ID，可以快速找到他最相似的 k 个用户

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315208-7cd72b7f-013a-4175-aeaf-639c69420a95.png)![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315294-05ee2d9c-ff2f-4823-a139-2e262543263a.png)

- 线上做召回

1. 1. 给定用户 ID，通过“用户 → 用户”索引，找到 top-k 相似用户
   2. 对于每个 top-k 相似用户，通过“用户 → 物品”索引，找到用户近期感兴趣的物品列表（last-n）
   3. 对于取回的 nk 个相似物品，用公式预估用户对每个物品的兴趣分数
   4. 返回分数最高的 100 个物品，作为召回结果

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315385-b370a132-0bcd-4714-8149-c508af24d51f.png)

- - 

## 离散特征处理

后面讲向量召回，需要用到这部分知识。

### 离散特征

- 举例：

- - 性别：男、女两种类别
  - 国籍：中国、美国、印度等 200 个国家
  - 英文单词：常见的英文单词有几万个
  - 物品 ID：小红书有几亿篇笔记，每篇笔记有一个 ID
  - 用户 ID：小红书有几亿个用户，每个用户有一个 ID

- 离散特征处理的步骤

1. 1. 建立字典：把类别映射成序号

- - - 中国 → 1
    - 美国 → 2
    - 印度 → 3

1. 1. 向量化：把序号映射成向量

- - - One-hot 编码：把序号映射成高维稀疏向量（向量中只有 0，1）
    - Embedding：把序号映射成低维稠密向量（向量中有小数）

### One-Hot 编码

- 例 1：性别特征

- - 性别：男、女两种类别
  - 字典：男 → 1，女 → 2
  - One-hot 编码：用 2 维向量表示性别

- - - 未知 → 0 → [0, 0]
    - 男    → 1 → [1, 0]
    - 女    → 2 → [0, 1]

- 例 2：国籍特征

- - 国籍：中国、美国、印度等 200 种类别
  - 字典：中国 → 1，美国 → 2，印度 → 3， ...
  - One-hot 编码：用 200 维稀疏向量表示国籍

- - - 未知 → 0 → [0, 0, 0, 0, ⋯ , 0]
    - 中国 → 1 → [1, 0, 0, 0, ⋯ , 0]
    - 美国 → 2 → [0, 1, 0, 0, ⋯ , 0]
    - 印度 → 3 → [0, 0, 1, 0, ⋯ , 0]

- One-Hot 编码的局限

- - 例 1：自然语言处理中，对单词做编码

- - - 英文有几万个常见单词
    - 那么 one-hot 向量的维度是几万，实践中一般不会用这么高纬的向量

- - 例 2：推荐系统中，对物品 ID 做编码

- - - 小红书有几亿篇笔记
    - 那么 one-hot 向量的维度是几亿

- - 类别数量太大时，通常不用 one-hot 编码

### Embedding（嵌入）

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315474-794bda3a-8cb3-4fbc-b68b-54d9517efbfb.png)

- 参数数量：向量维度 × 类别数量

- - 设 embedding 得到的向量都是 4 维的
  - 一共有 200 个国籍
  - 参数数量 = 4 × 200 = 800
  - 如果是onehot的话，那参数需要 200 × 200 = 40000

- 编程实现：TensorFlow、PyTorch 提供 embedding 层

- - 参数以矩阵的形式保存，矩阵大小是 向量维度 × 类别数量
  - 输入是序号，比如“美国”的序号是 2
  - 输出是向量，比如“美国”对应参数矩阵的第 2 列

- 一个神经网络绝大多数的参数都在 Embedding 层
- 所以工业界都会对 Embedding 层做很多优化，以提高存储和计算的效率

<img src="https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315571-3c0c56b8-4ddf-492e-9b67-e709beaf7df3.png" alt="img" style="zoom:50%;" />

<img src="https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315677-b4f95767-1df5-4f3e-9827-eaef2abb582c.png" alt="img" style="zoom:50%;" />

这里embedding layer的参数是一个矩阵，然后对应的输出是矩阵中的某一列，矩阵中列的数量   是品类的数量

- 

## 矩阵补充

- 矩阵补充是向量召回最常见的方法，只是现在不常用了
- 此处讲解矩阵补充，是为了帮助理解下节课的双塔模型

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315791-5bfc7658-6112-4537-8a28-4d1085472232.png)

矩阵补充模型：基于 Embedding 来做推荐。

### 训练

- 基本想法

- - 用户 embedding 参数矩阵记作 $\bold{A}$。第 $u$ 号用户对应矩阵第 $u$ 列，记作向量 $\bold{a}_u$
  - 物品 embedding 参数矩阵记作 $\bold{B}$。第$ i $号物品对应矩阵第 $i$ 列，记作向量 $\bold{b}_i$
  - 内积 $\left\langle \bold{a}_u,\bold{b}_i \right\rangle$ 是第$ u $号用户对第 $i $号物品兴趣的预估值
  - 训练模型的目的是学习矩阵 $\bold{A}$ 和 $\bold{B}$，使得预估值拟合真实观测的兴趣分数

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042315908-e52971dc-adb4-4fe8-a7cd-95e25d7703fc.png)

- 数据集

- - 数据集：（用户 ID，物品 ID，真实兴趣分数）的集合，记作 $\Omega=\{(u,i,y) \}$
  - 数据集中的兴趣分数是系统记录的，比如：

- - - 曝光但是没有点击 → 0 分
    - 点击、点赞、收藏、转发 → 各算 1 分
    - 分数最低是 0，最高是 4

- - 训练的目的就是让模型的输出拟合真实兴趣分数

- 训练

- - 把用户 ID、物品 ID 映射成向量。

- - - 第$ u$ 号用户 → 向量$ \bold{a}_u$
    - 第$ i $号物品 → 向量$ \bold{b}_i$

- - 求解优化问题，得到参数 $\bold{A}$ 和$ \bold{B}$:  $\min_{\bold{A},\bold{B}}\sum_{(u,i,y)\in\Omega}(y- \left\langle \bold{a}_u,\bold{b}_i \right\rangle)^2$

- - - 找到使得 真实兴趣分数 y 与 模型输出$ \left\langle \bold{a}_u,\bold{b}_i \right\rangle $间 差别 最小的 $\bold{A} $和$ \bold{B}$
    - 求最小化常用的方法就是随机梯度下降，每次更新矩阵 $\bold{A}$ 和$ \bold{B}$ 的一列

- 解释下为什么模型叫做矩阵补充

- - ![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042316018-1bf99117-9105-4290-9997-5fd589d5ba94.png)

- - - 绿色位置表示曝光给用户的物品；灰色位置表示没有曝光

- - 矩阵中只有少数位置是绿色，大多数位置是灰色（即大部分物品没有曝光给用户）
  - 而我们用绿色位置训练出的模型，可以预估所有灰色位置的输出，即把矩阵的元素补全
  - 把矩阵元素补全后，我们只需选出对应用户一行中分数较高的 物品 推荐给 用户 即可

- 矩阵补充在实践中效果不好 ……

- - 缺点 1：仅用 ID embedding，没利用物品、用户属性

- - - 物品属性：类目、关键词、地理位置、作者信息
    - 用户属性：性别、年龄、地理定位、感兴趣的类目
    - 双塔模型可以看做矩阵补充的升级版

- - - - 双塔模型不仅使用 ID，还结合各种属性

- - 缺点 2：负样本的选取方式不对

- - - 样本：用户—物品的二元组，记作$ (u,i)$
    - 正样本：曝光之后，有点击、交互。（正确的做法）
    - 负样本：曝光之后，没有点击、交互。（错误的做法）

- - - - 后面会专门用一节课时间讲正负样本如何选择

- - 缺点 3：做训练的方法不好

- - - 内积$ \left\langle \bold{a}_u,\bold{b}_i \right\rangle $不如余弦相似度

- - - - 工业界普遍使用 余弦相似度 而不是 内积

- - - 用平方损失（回归），不如用交叉熵损失（分类）

### 线上服务

- 模型存储

1. 1. 训练得到矩阵$ \bold{A} $和 $\bold{B}$

- - - $\bold{A}$ 的每一列对应一个用户
    - $\bold{B} $的每一列对应一个物品

1. 1. 把矩阵$ \bold{A}$ 的列存储到 key-value 表

- - - key 是用户 ID，value 是$ \bold{A}$ 的一列
    - 给定用户 ID，返回一个向量（用户的 embedding）

1. 1. 矩阵$ \bold{B}$ 的存储和索引比较复杂

- 线上服务

1. 1. 把用户 ID 作为 key，查询 key-value 表，得到该用户的 embedding 向量，记作 \bold{a}
   2. 最近邻查找：查找用户最有可能感兴趣的 k 个物品，作为召回结果

- - - 第 𝑖 号物品的 embedding 向量记作$ \bold{b}_i$
    - 内积 $\left\langle \bold{a}_u,\bold{b}_i \right\rangle$ 是用户对第 𝑖 号物品兴趣的预估
    - 返回内积最大的 k 个物品

- 如果枚举所有物品，时间复杂度正比于物品数量
- 最近邻查找的计算量太大，不现实，下面讲解如何加速最近邻查找

### 近似最近邻查找

- 支持最近邻查找的系统

- - 系统：Milvus、Faiss、HnswLib、等等

- - - 快速最近邻查找的算法已经被集成到这些系统中

- - 衡量最近邻的标准：

- - - 欧式距离最小（L2 距离）
    - 向量内积最大（内积相似度）

- - - - 矩阵补充用的就是内积相似度

- - - 向量夹角余弦最大（cosine 相似度）

- - - - 最常用
      - 对于不支持的系统：把所有向量作归一化（让它们的二范数等于 1），此时内积就等于余弦相似度

<img src="https://cdn.nlark.com/yuque/0/2022/png/101969/1672042316143-7b94cbd6-db64-4ce6-8739-b7070b471431.png" alt="img" style="zoom:50%;" />

- 图中每个点表示一个物品的 Embedding
- 右边的 ★ 表示用户$ \bold{a}$
- 数据预处理：把数据据划分为多个区域

- - 划分后，每个区域用一个向量表示，这些向量的长度都是 1
  - 例如图中蓝色区域用蓝色箭头向量表示
  - <img src="https://cdn.nlark.com/yuque/0/2022/png/101969/1672042316265-f9bdddd9-6a26-4c0c-b9cb-033cf5e8b18d.png" alt="img" style="zoom:50%;" />

- 直至每个区域都用一个单位向量来表示

- - 这些单位向量又称为 索引向量
  - <img src="https://cdn.nlark.com/yuque/0/2022/png/101969/1672042316397-ffc052b7-a1d1-49fa-bbae-d766582e91e2.png" alt="img" style="zoom:50%;" />

- 实际推荐时，先把用户向量与所有的索引向量做对比

- - 计算相似度，找到最相似的索引向量
  - <img src="https://cdn.nlark.com/yuque/0/2022/png/101969/1672042316503-39e31422-ab4e-4d1f-9c9c-95c492ed675d.png" alt="img" style="zoom:50%;" />

- 通过索引向量，我们找到索引对应区域中的所有物品，然后再计算该区域中所有物品与 $\bold{a} $的相似度

- - 一般情况是几亿个物品被几万个索引向量划分，于是一个区域中就只有几万个物品
  - <img src="https://cdn.nlark.com/yuque/0/2022/png/101969/1672042316599-f6f2d111-6db1-40cf-adee-f7183ad8ef92.png" alt="img" style="zoom:50%;" />

### 总结

- 矩阵补充

- - 把物品 ID、用户 ID 做 embedding，映射成向量
  - 两个向量的内积$ \left\langle \bold{a}_u,\bold{b}_i \right\rangle $作为用户 u 对物品$ i$ 兴趣的预估值
  - 训练时让$ \left\langle \bold{a}_u,\bold{b}_i \right\rangle $拟合真实观测的兴趣分数，学习模型的 embedding 层参数
  - 矩阵补充模型有很多缺点，效果不好

- 线上召回

- - 把用户向量 $\bold{a}$ 作为 query，查找使得  $\left\langle \bold{a}_u,\bold{b}_i \right\rangle$  最大化的物品$ i$
  - 暴力枚举速度太慢。实践中用近似最近邻查找
  - Milvus、Faiss、HnswLib 等向量数据库支持近似最近邻查找

## 双塔模型：模型和训练

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042316690-403d8462-f026-4de3-a43e-d99119362d01.png)

### 双塔模型

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042316812-3bce2474-8eda-48cb-aa80-6f58bb78b238.png)![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042316930-512c4448-5964-4bf2-9309-419cd3a16db5.png)

- 用户离散特征：例如所在城市、感兴趣的话题等

- - 对每个离散特征，单独使用一个 Embedding 层得到一个向量
  - 对于性别这种类别很少的离散特征，直接用 one-hot 编码

- 用户连续特征：年龄、活跃程度、消费金额等

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042317078-7e47cf8c-e659-4582-8ea5-4b131b39913f.png)

- 双塔模型：左塔提取用户特征，右塔提取物品特征

- - 与矩阵补充的区别在于，使用了除 ID 外的多种特征作为双塔的输入

- 双塔模型的训练

- - Pointwise：独立看待每个正样本、负样本，做简单的二元分类
  - Pairwise：每次取一个正样本、一个负样本
  - Listwise：每次取一个正样本、多个负样本，通常从一个batch的数据中产生n(n-1)个list

- - 

### Pointwise 训练

- 把召回看做二元分类任务
- 对于正样本，鼓励$ \cos{(\bold{a},\bold{b})} $接近 +1
- 对于负样本，鼓励 $\cos{(\bold{a},\bold{b})}$ 接近 −1
- 控制正负样本数量为 1: 2 或者 1: 3

### Pairwise 训练

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042317230-6a26dfa7-08dd-4a8b-a7c0-532855e6168f.png)

- 两个物品塔是相同的，它们共享参数

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042317349-25d382e4-8c3f-4881-984e-7348d3173963.png)

- 基本想法：鼓励$ \cos{(\bold{a},\bold{b}^+)}$ 大于$ \cos{(\bold{a},\bold{b}^-)}$

- - 如果  $\cos{(\bold{a},\bold{b}^+)} 大于  \cos{(\bold{a},\bold{b}^-)}+m$，则没有损失

- - - m 是超参数，需要调

- - 否则，损失等于$  \cos{(\bold{a},\bold{b}^-)}+m-\cos{(\bold{a},\bold{b}^+)}$

- **Triplet hinge loss**:
  $L(\bold{a},\bold{b}^+,\bold{b}^-)=\max{\{ 0,\cos{(\bold{a},\bold{b}^-)}+m-\cos{(\bold{a},\bold{b}^+)}\}}$
- **Triplet logistic loss:**

- - $L(\bold{a},\bold{b}^+,\bold{b}^-)=\log{( 1+\exp{[\sigma·(\cos{(\bold{a},\bold{b}^-)}-\cos{(\bold{a},\bold{b}^+))}])}}$

- - $\sigma $是大于 0 的超参数，控制损失函数的形状，需手动设置

### Listwise 训练

- 一条数据包含：

- - 一个用户，特征向量记作 $\bold{a}$
  - 一个正样本，特征向量记作 $\bold{b}^+$
  - 多个负样本，特征向量记作 $\bold{b}^-_1,...,\bold{b}^-_n$

- 鼓励 $\cos{(\bold{a},\bold{b}^+)}$ 尽量大
- 鼓励 $\cos{(\bold{a},\bold{b}^-_1)},...,\cos{(\bold{a},\bold{b}^-_n)}$ 尽量小

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042317453-2e6860cf-79ad-492a-b350-5f8115174edf.png)

- 正样本 $y^+=1$，即鼓励 $s^+ $趋于 1
- 负样本 $y^-_1=...=y^-_n=0$，即鼓励 $s^-_1...s^-_n $趋于 0
- 用 y 和 s 的交叉熵作为损失函数，意思是鼓励 $\rm Softmax$ 的输出 s 接近标签 y

### 总结

- 双塔模型

- - 用户塔、物品塔各输出一个向量
  - 两个向量的余弦相似度作为兴趣的预估值
  - 三种训练方式：

- - - Pointwise：每次用一个用户、一个物品（可正可负）
    - Pairwise：每次用一个用户、一个正样本、一个负样本
    - Listwise：每次用一个用户、一个正样本、多个负样本

- 不适用于召回的模型

- - ![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042317590-889ebdfc-4e88-4b03-bfd7-57e946cbdc3f.png)
  - 用户和物品的向量在进入神经网络前就拼接起来了，和双塔模型有很大区别

- - - 双塔模型是在后期输出相似度时才进行融合
    - 用户（或物品）自身特征的拼接没有影响，依然保持了用户（或物品）的独立性

- - - - 而一旦用户和物品进行拼接，此时的输出就特定于该 用户（或物品）了

- - 这种前期融合的模型，不适用于召回

- - - 因为得在召回前，把每个用户向量对应的所有物品向量挨个拼接了送入神经网络
    - 假设有一亿个物品，每给用户做一次召回，就得跑一亿遍

- - 这种模型通常用于排序，在几千个候选物品中选出几百个
  - 以后看到这种模型就要意识到 —— 这是排序模型，不是召回模型

## 双塔模型：正负样本

### 正样本

- 正样本：曝光而且有点击的 用户—物品 二元组（用户对物品感兴趣）
- 问题：少部分物品占据大部分点击，导致正样本大多是热门物品
- 解决方案：过采样冷门物品，或降采样热门物品

- - 过采样（up-sampling）：一个样本出现多次
  - 降采样（down-sampling）：一些样本被抛弃

- - - 以一定概率抛弃热门物品，抛弃的概率与样本的点击次数正相关

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042317744-98f7f9f0-abad-4634-9feb-2f567ab0f9fd.png)

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042317852-e1721f7f-f1cb-4910-af2f-0847eaf52b1b.png)

### 简单负样本

- 简单负样本：全体物品

- - 未被召回的物品，大概率是用户不感兴趣的
  - 未被召回的物品 ≈ 全体物品
  - 从全体物品中做抽样，作为负样本
  - 均匀抽样 or 非均匀抽样？

- 均匀抽样：对冷门物品不公平

- - 正样本大多是热门物品
  - 如果均匀抽样产生负样本，负样本大多是冷门物品

- 非均抽采样：目的是打压热门物品

- - 负样本抽样概率与热门程度（点击次数）正相关
  - 抽样概率 ∝$ (点击次数)^{0.75}$

- 简单负样本：Batch 内负样本

- - ![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042318032-e4d044d2-fbe2-48c6-9187-9e896e7e7016.png)![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042318149-0b62c2f5-9601-4cf0-a8ba-b1df4875ddd0.png)
  - 一个 batch 内有 n 个正样本
  - 一个用户和 n-1 个物品组成负样本
  - 这个 batch 内一共有 n(n-1) 个负样本
  - 都是简单负样本。（因为第一个用户不喜欢第二个物品）
  - Batch 内负样本存在的问题

- - - 一个物品出现在 batch 内的概率 ∝ 点击次数
    - 物品成为负样本的概率本该是 ∝ $ (点击次数)^{0.75}$，但这里实际是 ∝ 点击次数
    - 热门物品成为负样本的概率过大

- - - - 即对热门物品打压太狠了，容易造成偏差，下面这篇论文讲了如何修正偏差

参考文献：Xinyang Yi et al. Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations. In RecSys, 2019.

- 修正偏差：

- - 物品 i 被抽样到的概率：$p_i$ ∝ 点击次数
  - 预估用户对物品 i 的兴趣：$\cos{(\bold{a},\bold{b}_i)}$
  - 做训练的时候，将兴趣调整为：$\cos{(\bold{a},\bold{b}_i)}-\log{p_i}$

- - - 这样纠偏，避免过度打压热门物品
    - 训练结束后，在线上做召回时，还是用$ \cos{(\bold{a},\bold{b}_i)}$ 作为兴趣

### 困难负样本

- 困难负样本：

- - 被粗排淘汰的物品（比较困难）

- - - 这些物品被召回，说明和用户兴趣有关；又被粗排淘汰，说明用户对物品兴趣不大
    - 而在对正负样本做二元分类时，这些困难样本容易被分错（被错误判定为正样本）

- - 精排分数靠后的物品（非常困难）

- - - 能够进入精排，说明物品比较符合用户兴趣，但不是用户最感兴趣的

- 对正负样本做二元分类：

- - 全体物品（简单）分类准确率高
  - 被粗排淘汰的物品（比较困难）容易分错
  - 精排分数靠后的物品（非常困难）更容易分错

- 训练数据

- - 混合几种负样本
  - 50% 的负样本是全体物品（简单负样本）
  - 50% 的负样本是没通过排序的物品（困难负样本）

- - - 即在粗排、精排淘汰的物品

### 常见的错误

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042318236-d34cc538-6373-4de9-a53c-21c2eb6173a6.png)

- 选择负样本的原理

- - 召回的目标：快速找到用户可能感兴趣的物品

- - - 即区分用户 **不感兴趣** 和 **可能感兴趣** 的物品，而不是区分 **比较感兴趣** 和 **非常感兴趣** 的物品

- - 全体物品（easy ）：绝大多数是用户根本不感兴趣的
  - 被排序淘汰（hard ）：用户可能感兴趣，但是不够感兴趣
  - 有曝光没点击（没用）：用户感兴趣，可能碰巧没有点击

- - - 曝光没点击的物品已经非常符合用户兴趣了，甚至可以拿来做召回的正样本
    - 可以作为排序的负样本，不能作为召回的负样本

### 总结

- 正样本： 曝光而且有点击
- 简单负样本：

- - 全体物品
  - Batch 内负样本

- 困难负样本：被召回，但是被排序淘汰
- 错误：曝光、但是未点击的物品做召回的负样本

## 双塔模型：线上召回和更新

### 线上召回

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042318339-77721c1a-e3b9-4240-a57b-479dc9417287.png)![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042318449-365243b3-52b6-4ad2-ac6f-7fe09a5854a9.png)

- 双塔模型的召回

- - 离线存储：把物品向量 $\bold{b}$ 存入向量数据库

1. 1. 1. 完成训练之后，用物品塔计算每个物品的特征向量$ \bold{b}$
      2. 把几亿个物品向量 $\bold{b} $存入向量数据库（比如 Milvus、Faiss、HnswLib ）
      3. 向量数据库建索引，以便加速最近邻查找

- - 线上召回：查找用户最感兴趣的 k 个物品

1. 1. 1. 给定用户 ID 和画像，线上用神经网络现算（**实时计算**）用户向量 $\bold{a}$
      2. 最近邻查找：

- - - - 把向量$ \bold{a}$ 作为 query，调用向量数据库做最近邻查找
      - 返回余弦相似度最大的 k 个物品，作为召回结果

- 为什么事先存储物品向量 $\bold{b}$，线上现算用户向量 $\bold{a}$？

- - 每做一次召回，用到一个用户向量$ \bold{a}$，几亿物品向量 $\bold{b}$（线上算物品向量的代价过大）
  - 用户兴趣动态变化，而物品特征相对稳定（可以离线存储用户向量，但不利于推荐效果）

### 模型更新

- 全量更新 vs 增量更新

- - 全量更新：今天凌晨，用昨天全天的数据训练模型

- - - 在昨天模型参数的基础上做训练（不是重新随机初始化）
    - 用昨天的数据，训练 1 epoch，即每天数据只用一遍
    - 发布新的 用户塔神经网络 和 物品向量，供线上召回使用
    - 全量更新对数据流、系统的要求比较低

- - 增量更新：做 online learning 更新模型参数

- - - 用户兴趣会随时发生变化
    - 实时收集线上数据，做流式处理，生成 TFRecord 文件
    - 对模型做 online learning，增量更新 ID Embedding 参数（不更新神经网络其他部分的参数）

- - - - 即锁住全连接层的参数，只更新 Embedding 层的参数，这是出于工程实现的考量

- - - 发布用户 ID Embedding，供用户塔在线上计算用户向量

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042318550-93ac8230-4a4e-44b0-824a-852475c1925d.png)

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042318647-8b5ba9e7-e9a1-4ce8-bb89-5d9a14fb364e.png)

- 问题：能否只做增量更新，不做全量更新？

- - 小时级数据有偏；分钟级数据偏差更大

- - - 在不同时间段，用户行为不一样，这和全体数据的统计值差别很大

- - 全量更新：random shuffle 一天的数据，做 1 epoch 训练

- - - random shuffle 消除了不同时间段的差别

- - 增量更新：按照数据从早到晚的顺序，做 1 epoch 训练
  - **随机打乱**优于**按顺序排列数据**，全量训练优于增量训练

### 总结

- 双塔模型

- - 用户塔、物品塔各输出一个向量，两个向量的余弦相似度作为兴趣的预估值
  - 三种训练的方式：pointwise、pairwise、listwise
  - 正样本：用户点击过的物品
  - 负样本：全体物品（简单）、被排序淘汰的物品（困难）

- 召回

- - 做完训练，把物品向量存储到向量数据库，供线上最近邻查找
  - 线上召回时，给定用户 ID、用户画像，调用用户塔现算用户向量$ \bold{a}$
  - 把 $\bold{a}$ 作为 query，查询向量数据库，找到余弦相似度最高的 k 个物品向量，返回 k 个物品 ID

- 更新模型

- - 全量更新：今天凌晨，用昨天的数据训练整个神经网络，做 1 epoch 的随机梯度下降
  - 增量更新：用实时数据训练神经网络，只更新 ID Embedding，锁住全连接层
  - 实际的系统：

- - - 全量更新 & 增量更新 相结合
    - 每隔几十分钟，发布最新的用户 ID Embedding，供用户塔在线上计算用户向量

## 其他召回通道

### Deep Retrieval

本质：是用**路径**来作为用户和物品之间的中介，用户→路径→物品

和双塔模型的区别？ deep retrieval使用路径来作为中介；双塔模型采用向量表征来作为中介

beam search ：用来帮助快速找出用户较为感兴趣的路径。 beam size ，是一种贪心策略的算法

### 地理位置召回

- GeoHash 召回

- - 用户可能对附近发生的事感兴趣
  - GeoHash：对经纬度的编码，大致表示地图上一个长方形区域
  - 索引：GeoHash → 优质笔记列表（按时间倒排）
  - 这条召回通道没有个性化（完全不考虑用户兴趣）

![img](https://cdn.nlark.com/yuque/0/2022/png/101969/1672042318740-fbad5f64-88ed-4abf-b630-999a4bd566bb.png)

根据用户定位的 GeoHash，取回该地点最新的 k 篇优质笔记

- 同城召回

- - 用户可能对同城发生的事感兴趣
  - 索引： 城市 → 优质笔记列表（按时间倒排）
  - 这条召回通道没有个性化

### 作者召回

- 关注作者召回

- - 用户对关注的作者发布的笔记感兴趣
  - 索引：

- - - 用户 → 关注的作者
    - 作者 → 发布的笔记

- - 召回：

- - - 用户 → 关注的作者 → 最新的笔记

- 有交互的作者召回

- - 如果用户对某笔记感兴趣（点赞、收藏、转发），那么用户可能对该作者的其他笔记感兴趣
  - 索引： 用户 → 有交互的作者

- - - 作者列表需要定期更新，加入最新交互的作者，删除长期未交互的作者

- - 召回： 用户 → 有交互的作者 → 最新的笔记

- 相似作者召回

- - 如果用户喜欢某作者，那么用户喜欢相似的作者
  - 索引：作者 → 相似作者（k 个作者）

- - - 作者相似度的计算类似于 ItemCF 中判断两个物品的相似度
    - 例如两个作者的粉丝有很大重合，则认定两个作者相似

- - 召回：用户 → 感兴趣的作者 → 相似作者 → 最新的笔记
    （n 个作者）  （nk 个作者）（nk 篇笔记）

### 缓存召回

- 缓存召回

- - 想法：复用前 n 次推荐精排的结果
  - 背景：

- - - 精排输出几百篇笔记，送入重排
    - 重排做多样性抽样，选出几十篇
    - 精排结果一大半没有曝光，被浪费

- - 精排前 50，但是没有曝光的，缓存起来，作为一条召回通道

- 缓存大小固定，需要退场机制

- - 一旦笔记成功曝光，就从缓存退场
  - 如果超出缓存大小，就移除最先进入缓存的笔记
  - 笔记最多被召回 10 次，达到 10 次就退场
  - 每篇笔记最多保存 3 天，达到 3 天就退场
  - 保护低曝光的笔记，低曝光的就多保留几天

### 总结

- 地理位置召回：

- - GeoHash 召回、同城召回

- 作者召回：

- - 关注的作者、有交互的作者、相似的作者

- 缓存召回

这 6 条召回通道都是工业界在用的，只是它们的重要性比不上 ItemCF、Swing、双塔那些通道。



## 曝光过滤 & Bloom Filter

在推荐系统中，如果用户看过某个物品，就不再把物品推荐给这个用户。小红书、抖音都这样做曝光过滤，原因是实验表明重复曝光同一个物品会损害用户体验。但也不是所有推荐系统都有曝光过滤，像 YouTube 这样的长视频就没有曝光过滤，看过的可以再次推荐。 曝光过滤通常是在召回阶段做。想要做曝光过滤，需要对于每个用户，记录已经曝光给他的物品。一个用户历史上看过的物品可能会非常多，为了做到高效的曝光过滤，需要用Bloom Filter这种数据结构。

### 问题定义

 曝光过滤解决的问题：看过的物品不再推荐给用户去曝光



### Bloom Filter

- 如果Bloom Filter说 no， 那么一定没曝光
- 如果说yes，那么大概率被曝光了



用k个哈希函数去把物品的ID映射到m维的二进制向量中

![image-20240310165326509](https://s2.loli.net/2024/03/10/KA3Mtc1BHkm6Qqu.png)



误伤的概率

<img src="https://s2.loli.net/2024/03/10/ehbnViZ5PB3YorT.png" alt="image-20240310170020908" style="zoom:67%;" />

<img src="https://s2.loli.net/2024/03/10/cQ2JVuXOL1CHhvt.png" alt="image-20240310170156550" style="zoom:67%;" />



### 曝光过滤的链路



![image-20240310170443950](https://s2.loli.net/2024/03/10/YRDhVtjEaicmLWv.png)

### 缺点

总结：无法删除物品

- Bloom filter 把物品的集合表示成一个二进制向量。
- 每往集合中添加一个物品，只需要把向量k个位置的元素置为1。(如果原本就是1，则不变。)
- Bloom filter 只支持添加物品，不支持删除物品。从集合中移除物品，无法消除它对向量的影响。
- 每天都需要从物品集合中移除年龄大于1个月的物品。(超龄物品不可能被召回，没必要把它们记录在Bloom filter，降低n可以降低误伤率。)
