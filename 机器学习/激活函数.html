<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>激活函数 | JACKY</title>
    <meta name="generator" content="VuePress 1.9.7">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
    <link rel="stylesheet" href="/css/index.css">
    <link rel="icon" href="/images/logo.png">
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.181f12b1.css" as="style"><link rel="preload" href="/assets/js/app.f60bcc2c.js" as="script"><link rel="preload" href="/assets/js/2.bad792ca.js" as="script"><link rel="preload" href="/assets/js/30.04c71f4a.js" as="script"><link rel="prefetch" href="/assets/js/10.3870dfc9.js"><link rel="prefetch" href="/assets/js/100.33789d0d.js"><link rel="prefetch" href="/assets/js/101.720e9545.js"><link rel="prefetch" href="/assets/js/102.0d46e59f.js"><link rel="prefetch" href="/assets/js/103.cf779584.js"><link rel="prefetch" href="/assets/js/104.c9ebede1.js"><link rel="prefetch" href="/assets/js/105.320a9546.js"><link rel="prefetch" href="/assets/js/106.e6805173.js"><link rel="prefetch" href="/assets/js/11.a6ea81b6.js"><link rel="prefetch" href="/assets/js/12.4bb4b38d.js"><link rel="prefetch" href="/assets/js/13.a8729ae5.js"><link rel="prefetch" href="/assets/js/14.88f89f42.js"><link rel="prefetch" href="/assets/js/15.71e61a0e.js"><link rel="prefetch" href="/assets/js/16.7267174c.js"><link rel="prefetch" href="/assets/js/17.4f829a89.js"><link rel="prefetch" href="/assets/js/18.03ec02b6.js"><link rel="prefetch" href="/assets/js/19.84bfd456.js"><link rel="prefetch" href="/assets/js/20.3b51b944.js"><link rel="prefetch" href="/assets/js/21.186891ef.js"><link rel="prefetch" href="/assets/js/22.fee81424.js"><link rel="prefetch" href="/assets/js/23.d0aac5b9.js"><link rel="prefetch" href="/assets/js/24.6b69cfe2.js"><link rel="prefetch" href="/assets/js/25.c66af061.js"><link rel="prefetch" href="/assets/js/26.f6dbd423.js"><link rel="prefetch" href="/assets/js/27.4b120a99.js"><link rel="prefetch" href="/assets/js/28.0d925b0c.js"><link rel="prefetch" href="/assets/js/29.3b765d1c.js"><link rel="prefetch" href="/assets/js/3.b1f3d5f9.js"><link rel="prefetch" href="/assets/js/31.3fd9262e.js"><link rel="prefetch" href="/assets/js/32.181e3c1d.js"><link rel="prefetch" href="/assets/js/33.b55218e6.js"><link rel="prefetch" href="/assets/js/34.c495c048.js"><link rel="prefetch" href="/assets/js/35.eb762b8c.js"><link rel="prefetch" href="/assets/js/36.63ad4fbe.js"><link rel="prefetch" href="/assets/js/37.8663a4c1.js"><link rel="prefetch" href="/assets/js/38.2d05d18d.js"><link rel="prefetch" href="/assets/js/39.efc12a85.js"><link rel="prefetch" href="/assets/js/4.c0e92c37.js"><link rel="prefetch" href="/assets/js/40.925f57db.js"><link rel="prefetch" href="/assets/js/41.b3c0c5bb.js"><link rel="prefetch" href="/assets/js/42.1c1f9d9a.js"><link rel="prefetch" href="/assets/js/43.43c0bd60.js"><link rel="prefetch" href="/assets/js/44.c6df2236.js"><link rel="prefetch" href="/assets/js/45.1741c663.js"><link rel="prefetch" href="/assets/js/46.57e5bc75.js"><link rel="prefetch" href="/assets/js/47.9b059617.js"><link rel="prefetch" href="/assets/js/48.cd1e17f8.js"><link rel="prefetch" href="/assets/js/49.34700620.js"><link rel="prefetch" href="/assets/js/5.762ede0d.js"><link rel="prefetch" href="/assets/js/50.75eaa7f3.js"><link rel="prefetch" href="/assets/js/51.a40454fa.js"><link rel="prefetch" href="/assets/js/52.d5f7c3a5.js"><link rel="prefetch" href="/assets/js/53.d1164390.js"><link rel="prefetch" href="/assets/js/54.056dd2b2.js"><link rel="prefetch" href="/assets/js/55.190196c0.js"><link rel="prefetch" href="/assets/js/56.b548448c.js"><link rel="prefetch" href="/assets/js/57.96f33ef6.js"><link rel="prefetch" href="/assets/js/58.d2d99324.js"><link rel="prefetch" href="/assets/js/59.42c4e143.js"><link rel="prefetch" href="/assets/js/6.eb959d0b.js"><link rel="prefetch" href="/assets/js/60.caa709e5.js"><link rel="prefetch" href="/assets/js/61.0d4d2549.js"><link rel="prefetch" href="/assets/js/62.4160b095.js"><link rel="prefetch" href="/assets/js/63.ec1f97e1.js"><link rel="prefetch" href="/assets/js/64.204c4d91.js"><link rel="prefetch" href="/assets/js/65.35e3e36c.js"><link rel="prefetch" href="/assets/js/66.22a4c023.js"><link rel="prefetch" href="/assets/js/67.99bf4edf.js"><link rel="prefetch" href="/assets/js/68.e7d4433f.js"><link rel="prefetch" href="/assets/js/69.3ad43e4a.js"><link rel="prefetch" href="/assets/js/7.02f12834.js"><link rel="prefetch" href="/assets/js/70.8a867480.js"><link rel="prefetch" href="/assets/js/71.81dd9524.js"><link rel="prefetch" href="/assets/js/72.ab573144.js"><link rel="prefetch" href="/assets/js/73.495bc942.js"><link rel="prefetch" href="/assets/js/74.d823be33.js"><link rel="prefetch" href="/assets/js/75.99f73d3f.js"><link rel="prefetch" href="/assets/js/76.dd1da660.js"><link rel="prefetch" href="/assets/js/77.3e5e36a8.js"><link rel="prefetch" href="/assets/js/78.051f8446.js"><link rel="prefetch" href="/assets/js/79.2d4b4180.js"><link rel="prefetch" href="/assets/js/8.e564c2ce.js"><link rel="prefetch" href="/assets/js/80.b26f37e5.js"><link rel="prefetch" href="/assets/js/81.628c5e4f.js"><link rel="prefetch" href="/assets/js/82.14b92cb9.js"><link rel="prefetch" href="/assets/js/83.695e8ef3.js"><link rel="prefetch" href="/assets/js/84.2dac3d18.js"><link rel="prefetch" href="/assets/js/85.80b287fd.js"><link rel="prefetch" href="/assets/js/86.b79b0692.js"><link rel="prefetch" href="/assets/js/87.aa942b96.js"><link rel="prefetch" href="/assets/js/88.ab91faa2.js"><link rel="prefetch" href="/assets/js/89.8b2cabda.js"><link rel="prefetch" href="/assets/js/9.d9599932.js"><link rel="prefetch" href="/assets/js/90.ad36d54b.js"><link rel="prefetch" href="/assets/js/91.9d368a46.js"><link rel="prefetch" href="/assets/js/92.47cf969d.js"><link rel="prefetch" href="/assets/js/93.bce74186.js"><link rel="prefetch" href="/assets/js/94.e74453fd.js"><link rel="prefetch" href="/assets/js/95.a7ef6466.js"><link rel="prefetch" href="/assets/js/96.2fcbed97.js"><link rel="prefetch" href="/assets/js/97.c51bd04f.js"><link rel="prefetch" href="/assets/js/98.5d1c6779.js"><link rel="prefetch" href="/assets/js/99.3d3196a0.js">
    <link rel="stylesheet" href="/assets/css/0.styles.181f12b1.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">JACKY</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <ul class="sidebar-links"><li><a href="/" aria-current="page" class="sidebar-link">/</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Linux操作系统</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>专业课</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>机器学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/机器学习/EM算法.html" class="sidebar-link">EM算法</a></li><li><a href="/机器学习/K近邻.html" class="sidebar-link">K-means</a></li><li><a href="/机器学习/NLP.html" class="sidebar-link">自然语言处理</a></li><li><a href="/机器学习/PCA.html" class="sidebar-link">PCA</a></li><li><a href="/机器学习/XGBoost.html" class="sidebar-link">XGBoost</a></li><li><a href="/机器学习/决策树.html" class="sidebar-link">决策树</a></li><li><a href="/机器学习/感知机.html" class="sidebar-link">感知机</a></li><li><a href="/机器学习/损失函数.html" class="sidebar-link">损失函数</a></li><li><a href="/机器学习/支持向量机.html" class="sidebar-link">SVM</a></li><li><a href="/机器学习/数据处理.html" class="sidebar-link">数据处理</a></li><li><a href="/机器学习/机器学习.html" class="sidebar-link">ML入门</a></li><li><a href="/机器学习/机器学习数学基础.html" class="sidebar-link">机器学习数学基础</a></li><li><a href="/机器学习/激活函数.html" class="active sidebar-link">激活函数</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/机器学习/激活函数.html#sigmoid" class="sidebar-link">sigmoid</a></li><li class="sidebar-sub-header"><a href="/机器学习/激活函数.html#tanh" class="sidebar-link">tanh</a></li><li class="sidebar-sub-header"><a href="/机器学习/激活函数.html#softmax" class="sidebar-link">softmax</a></li><li class="sidebar-sub-header"><a href="/机器学习/激活函数.html#relu" class="sidebar-link">ReLu</a></li></ul></li><li><a href="/机器学习/统计机器学习.html" class="sidebar-link">统计机器学习</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>深度学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>硬件设计</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>算法与数据结构</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>编程语言</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>联邦学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>软件开发</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="激活函数"><a href="#激活函数" class="header-anchor">#</a> 激活函数</h1> <p>作用：激活函数是用来加入非<a href="https://so.csdn.net/so/search?q=%E7%BA%BF%E6%80%A7&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">线性<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>因素的，解决线性模型所不能解决的问题</p> <blockquote><p>原因：由于机器学习做的事情其实都是线性运算（卷积、感知机、神经网络）</p> <p>不管模型有多少层，有多深，线性变换乘以线性变换，仍然是线性变换。</p></blockquote> <p>激活函数一般满足：</p> <blockquote><ol><li>函数本身是简单的</li> <li>函数的导函数也是简单的</li> <li>非线性： 激活函数为非线性激活函数的时候，基本上两层的神经网络就可以模拟大多数函数。但是如果没有激活函数的时候，对多层的神经网络来说只是做到了一个基本向量空间的线性变换，这与单层的神经网络是等价的。</li> <li>可微性： 在进行梯度优化和计算的时候，必须满足函数可微性的这一个条件以方便进行求导运算。</li> <li>单调性： 当激活函数是单调函数的时候，单层的神经网络能够保持是凸函数。</li> <li>f ( x ) ≈ x： 激活函数满足这个值主要为的是设置参数的初始化，以提高神经网络的训练效果。</li> <li>输出的范围： 激活函数输出范围值是一个重要的参数，<strong>当输出的范围是有限的时候，基于梯度的优化方法会更加稳定</strong>，因为特征量表示受到有限权值的影响会更加显著。当输出范围是一个无限值的时候，模型训练更加有效果。</li></ol></blockquote> <p><img src="https://img-blog.csdnimg.cn/20190719100438779.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5ODMxMTYz,size_16,color_FFFFFF,t_70" alt="img"></p> <ul><li><strong>饱和</strong>激活函数： sigmoid、 tanh</li> <li><strong>非饱和</strong>激活函数: ReLU 、Leaky Relu  、ELU【指数线性单元】、PReLU【<strong>参数化的</strong>ReLU 】、RReLU【随机ReLU】</li></ul> <p>相对于饱和激活函数，使用“<strong>非饱和激活函数”的优势</strong>在于两点：
1.首先，“非饱和激活函数”能解决深度神经网络【层数非常多！！】的“<strong>梯度消失”问题</strong>，浅层网络【三五层那种】才用<a href="https://so.csdn.net/so/search?q=sigmoid&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">sigmoid<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 作为激活函数。
2.其次，它能<strong>加快收敛速度</strong>。</p> <h2 id="sigmoid"><a href="#sigmoid" class="header-anchor">#</a> sigmoid</h2> <p>饱和时梯度值非常小。由于BP算法反向传播的时候后层的梯度是以乘性方式传递到前层，因此当层数比较多的时候，传到前层的梯度就会非常小，网络权值得不到有效的更新，即梯度耗散。如果该层的权值初始化使得f(x) 处于饱和状态时，网络基本上权值无法更新。</p> <p>目前已被淘汰，只适用于浅层网络。</p> <p>tanh特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果显示出来，但有是，在特征相差比较复杂或是相差不是特别大时，需要更细微的分类判断的时候，sigmoid效果就好了。
还有一个东西要注意，sigmoid 和 tanh作为激活函数的话，一定要注意一定要对 input 进行归一话，否则激活后的值都会进入<strong>平坦区</strong>，使隐层的输出全部趋同，但是 ReLU 并不需要输入归一化来防止它们达到饱和。</p> <h2 id="tanh"><a href="#tanh" class="header-anchor">#</a> tanh</h2> <p>tanh与sigmoid本质上是一样的，也存在饱和的问题。</p> <h2 id="softmax"><a href="#softmax" class="header-anchor">#</a> softmax</h2> <p>一般也不叫他激活函数，可以叫做输出函数。</p> <p>多用于多分类问题，在最后一层使用，用于计算输出<strong>每个结果的概率</strong>。</p> <p><img src="https://img-blog.csdn.net/20161219175058506" alt="img"></p> <h2 id="relu"><a href="#relu" class="header-anchor">#</a> ReLu</h2> <div class="custom-block tip"><p class="custom-block-title">优点</p> <p>第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。</p> <p>第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失，，从而无法完成深层网络的训练。</p> <p>第三，ReLu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。</p></div> <p>构建稀疏矩阵，也就是稀疏性，这个特性可以去除数据中的冗余，最大可能保留数据的特征，也就是大多数为0的稀疏矩阵来表示。其实这个特性主要是对于Relu，它就是取的max(0,x)，因为神经网络是不断反复计算，实际上变成了它在尝试不断试探如何用一个大多数为0的矩阵来尝试表达数据特征，结果因为稀疏特性的存在，反而这种方法变得运算得又快效果又好了。所以我们可以看到目前大部分的卷积神经网络中，基本上都是采用了ReLU 函数。</p> <p><strong>深度学习目前最常用的激活函数</strong></p> <p>与Sigmoid/tanh函数相比，ReLu激活函数的优点是：</p> <ul><li>使用梯度下降（GD）法时，收敛速度更快</li> <li>相比Relu只需要一个门限值，即可以得到激活值，计算速度更快</li></ul> <p>缺点是：Relu的输入值为负的时候，输出始终为0，其一阶导数也始终为0，这样会导致神经元不能更新参数，也就是神经元不学习了，这种现象叫做“Dead Neuron”。</p> <p>如果后层的某一个梯度特别大，导致W更新以后变得特别大，导致该层的输入&lt;0，输出为0，这时该层就会‘die’，没有更新。当学习率比较大时可能会有40%的神经元都会在训练开始就‘die’，因此需要对学习率进行一个好的设置。</p> <p>为了解决Relu函数这个缺点，在Relu函数的负半区间引入一个泄露（Leaky）值，所以称为Leaky Relu函数。</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/机器学习/机器学习数学基础.html" class="prev">
        机器学习数学基础
      </a></span> <span class="next"><a href="/机器学习/统计机器学习.html">
        统计机器学习
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.f60bcc2c.js" defer></script><script src="/assets/js/2.bad792ca.js" defer></script><script src="/assets/js/30.04c71f4a.js" defer></script>
  </body>
</html>
